{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_viz.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tguerand/SentimentAnalysis/blob/master/bin/data_viz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFZIYimCA6M2"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8sgvTGdAmBB",
        "outputId": "c4fba777-db93-4cf8-e295-bbd99b9ac34f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import os\n",
        "import urllib.request\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(\"Tensorflow version is :\", tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version is : 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtxPeEmpHIhN"
      },
      "source": [
        "#Access Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrACEbjWR4kT",
        "outputId": "41e7f11e-615c-45e5-e1ea-093650687d5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "!git clone https://github.com/tguerand/SentimentAnalysis\n",
        "os.mkdir(\"./SentimentAnalysis/DATA\")\n",
        "#Download file\n",
        "!gdown --id 1dO4aubOro159awBPzD675c22qtXMOVp0 --output ./SentimentAnalysis/DATA/training.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'SentimentAnalysis'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 71 (delta 24), reused 17 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (71/71), done.\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dO4aubOro159awBPzD675c22qtXMOVp0\n",
            "To: /content/SentimentAnalysis/DATA/training.csv\n",
            "239MB [00:02, 119MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLroE9p2rrSs"
      },
      "source": [
        "from SentimentAnalysis.bin import model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kU1Tz6zHrz8"
      },
      "source": [
        "#Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4I52KdZHzCa"
      },
      "source": [
        "df=pd.read_csv(\"./SentimentAnalysis/DATA/training.csv\",encoding='latin-1')\n",
        "df.columns=['target', 't_id', 'created_at', 'query', 'user', 'text']\n",
        "#df.head()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x8xZaI1BBTL",
        "outputId": "728e24be-c375-4e6d-a209-df973c4913d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "print(df[\"text\"].head(5).to_string())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    is upset that he can't update his Facebook by ...\n",
            "1    @Kenichan I dived many times for the ball. Man...\n",
            "2      my whole body feels itchy and like its on fire \n",
            "3    @nationwideclass no, it's not behaving at all....\n",
            "4                        @Kwesidei not the whole crew \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcNf4231eSZx"
      },
      "source": [
        "#Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLyxEimZc357"
      },
      "source": [
        "#### Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLB8t79uj1Gk"
      },
      "source": [
        "#code Micka\n",
        "\n",
        "df[\"nbhash\"]=df[\"text\"].str.count(\"#\")>0\n",
        "df[\"nbat\"]=df[\"text\"].str.count(\"@\")>0\n",
        "import re\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : re.sub('@[a-zA-Z0-9]+', 'ààà', x))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : re.sub('http://[a-zA-Z0-9]+', 'urlofsite', x))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : re.sub('www.[a-zA-Z0-9]+', 'urlofsite', x))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\"!\",\" exclamationmark \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\"?\",\" questionmark \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\"&lt;3\",\" loveemoji \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\":)\",\" smileemoji \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\": )\",\" smileemoji \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\":'(\",\" cryemoji \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\";)\",\" winkemoji \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\":/\",\" mehemoji \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\"w/o\",\" without \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\"b/c\",\" because \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\" w \",\" with \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\" ... \",\" pointdesuspension \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\" .. \",\" pointdesuspension \"))\n",
        "df[\"text\"] = df[\"text\"].apply(lambda x : x.replace(\" .... \",\" pointdesuspension \"))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsgpmUNCj-Ra"
      },
      "source": [
        "#truc à garder ou à tester :\n",
        "# les chiffres\n",
        "# les ! et ?\n",
        "# faire gaffe aux ', par exemple aren't ou didn't, qui expriment une négation\n",
        "# http://twitpic\n",
        "# http://bit\n",
        "# http://tinyurl\n",
        "# http://plurk\n",
        "# http://www\n",
        "# http://blip\n",
        "# &lt;3 qui vaut <3\n",
        "# :) ;) :/ mais aussi : ) ou :'(\n",
        "# &amp &gt et &quot qui ont l'air d'être des infos html\n",
        "# w/o without\n",
        "# w/ with\n",
        "# b/c because\n",
        "# ... .. et .... \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J611IpXEkLqh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIeTllDZkLzE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPMiTy1ORuSX"
      },
      "source": [
        "# Find which words are upper to remove ones whithout info ('I', ...)\n",
        "#df[\"text\"][0] = \"I AAA\"\n",
        "irrelevant_words = ['I']\n",
        "#df[\"redondant\"] = df[\"text\"].apply(lambda x: True if \"I\" in x.split() else False)\n",
        "x = df[\"text\"]\n",
        "for word in irrelevant_words:\n",
        "  x = x.str.replace(word,'')\n",
        "\n",
        "# Check if there is a UPPERCASE word in the series\n",
        "df[\"upper\"] = x.str.split()\n",
        "df[\"upper\"] = df[\"upper\"].apply(lambda x: [i.isupper() for i in x])\n",
        "df[\"upper\"] = df[\"upper\"].apply(lambda x: True if True in x else False)\n",
        "#df[\"upper\"].head()\n",
        "confusion_matrix = pd.crosstab(df[\"target\"], df[\"upper\"], rownames=['target'], colnames=['is any upper'])\n",
        "confusion_matrix/df.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWPd-jwVInD7",
        "outputId": "5c8903e4-6841-4b6c-8903-b416dde79266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#------------------------Eliminate stop words-------------------------------\n",
        "# Put everything to lower\n",
        "df[\"text\"]=df[\"text\"].str.lower()\n",
        "# remove stop_words\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "irrelevant_words = stopwords.words('english')\n",
        "irrelevant_words.append(\"i\\'m\")\n",
        "df['text'] = df[\"text\"].apply(lambda x: ' '.join([word for word in x.split() if word not in (irrelevant_words)]))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwK_Iv1Ge2tB",
        "outputId": "cd0ee70d-ee13-4cda-93c9-ceb6be30a99e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "#------------------------Data words Frequency-------------------------------\n",
        "treshhold=0.1\n",
        "dict_frequency=dict(tokenizer.word_counts)\n",
        "col=pd.Series(dict_frequency, name='freq')\n",
        "df_freq=pd.DataFrame(col.sort_values(ascending=False))\n",
        "df_freq.reset_index(drop=False)\n",
        "df_freq=df_freq[df_freq[\"freq\"]>treshhold]\n",
        "df_freq['freq']=df_freq['freq']/df_freq[\"freq\"].sum()*100\n",
        "#Verification\n",
        "#df_freq[\"freq\"].sum()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.00000000000003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zBY3YtIgAk2"
      },
      "source": [
        "print(df_freq.head(vocab_size).to_string())"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yY18tdIgFTp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y5jBnrXqHaG"
      },
      "source": [
        "#### With all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6Z9q775GniX"
      },
      "source": [
        "vocab_size = 1000\n",
        "max_len = max(len(train_padded[0]))\n",
        "\n",
        "sentences = df['text']\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<00V>\", filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n')\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index=tokenizer.word_index\n",
        "sequences=tokenizer.texts_to_sequences(sentences)\n",
        "padded=pad_sequences(sequences,padding=\"post\")"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAEoRoEbVYlE"
      },
      "source": [
        ""
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1KeZuGNqK3y"
      },
      "source": [
        "#### Spliting data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ju_bj2yChFT"
      },
      "source": [
        "vocab_size = 1000\n",
        "max_len = max(len(train_padded[0]))\n",
        "sentences = df['text']\n",
        "# Split Training and Testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df[\"target\"], test_size=0.33, random_state=42)\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<00V>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index=tokenizer.word_index\n",
        "\n",
        "train_sequences=tokenizer.texts_to_sequences(X_train)\n",
        "train_padded=pad_sequences(train_sequences,padding=\"post\")\n",
        "\n",
        "test_sequences=tokenizer.texts_to_sequences(X_test)\n",
        "test_padded=pad_sequences(test_sequences,padding=\"post\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxxtBeqRfEro"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoywpD3Xtc2Z"
      },
      "source": [
        "#### Model and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_YGs2voGUb3"
      },
      "source": [
        "embedding_dim = 16\n",
        "model=tf.keras.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Embedding(vocab_size,embedding_dim),\n",
        "     tf.keras.layers.GlobalAveragePooling1D(),\n",
        "     tf.keras.layers.Dense(24,activation=\"relu\"),\n",
        "     tf.keras.layers.Dense(1,activation=\"sigmoid\")\n",
        "    ]\n",
        "    )\n",
        "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhwhs3ZwRSfs"
      },
      "source": [
        "model.fit(train_padded,y_train,epochs=10,batch_size=100,validation_data=(test_padded,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UViyJ0a4rA3q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6trwdflrBxv"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxvScyuQtDZg"
      },
      "source": [
        "my_model = model.my_model(max_words=vocab_size, max_len=228, embedd_size=vocab_size)\n",
        "\n",
        "my_model.summary()\n",
        "\n",
        "my_model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43xvRGDuJq7"
      },
      "source": [
        "my_model.fit(train_padded, y_train, epochs=10, batch_size=256, validation_data=(test_padded,y_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}